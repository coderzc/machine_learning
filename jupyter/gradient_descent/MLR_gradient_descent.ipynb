{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元线性回归中的梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $-\\eta\\,\\nabla J=-\\eta \\, ( \\dfrac {\\partial J}{\\partial b},\\dfrac {\\partial J}{\\partial w_{1}},\\dfrac {\\partial J}{\\partial w_{2}} ,\\ldots ,\\dfrac {\\partial J}{\\partial w_{n}})$\n",
    " \n",
    " <img src=\"../img/2.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"../img/3.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666) # 设置随机种子\n",
    "x = 2 * np.random.random(size=100)\n",
    "y = x * 3. + 4. + np.random.normal(size=100) # np.random.normal(size=100) 为正态分布的噪音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统一化为二维数组 (里面有一个列向量)\n",
    "X = x.reshape(-1,1)\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11101b9b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用梯度下降法训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = (b;W) X_b = [[1,X1],[1,X2]....]\n",
    "def J(W,X_b,y):\n",
    "    try:\n",
    "        return np.sum((y-X_b.dot(W))**2) / len(y)\n",
    "    except:\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(W,X_b,y):\n",
    "    res=np.empty(len(W))\n",
    "    res[0] = np.sum((X_b.dot(W) - y))\n",
    "    for i in range(1,len(W)):\n",
    "         res[i] = np.sum((X_b.dot(W) - y).dot(X_b[:,i]))\n",
    "    return res * 2 / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(init_w,X,y,eta=0.01,epsilon=1e-8,n_iters=1e4,init_b=0.):\n",
    "    \n",
    "    # 补1\n",
    "    X_b = np.hstack([np.ones((len(X),1)),X])\n",
    "    # 补b\n",
    "    W = np.hstack([init_b,init_w])\n",
    "    \n",
    "    i_iter=0\n",
    "    while i_iter<n_iters:\n",
    "        grad=dJ(W,X_b,y)\n",
    "        last_W=W\n",
    "        W=W-eta*grad\n",
    "        if(abs(J(W,X_b,y)-J(last_W,X_b,y)) < epsilon):\n",
    "            break\n",
    "        i_iter += 1    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: 4.021457858204859\n",
      "w: [3.00706277]\n"
     ]
    }
   ],
   "source": [
    "init_w = np.zeros(X.shape[1])\n",
    "eta=0.01\n",
    "\n",
    "W_ = gradient_descent(init_w,X,y,eta)\n",
    "\n",
    "print(\"b:\",W_[0])\n",
    "print(\"w:\",W_[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降中的向量化和数据标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"../img/4.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla J=\\dfrac {2}{m}\\cdot X^{T}_{b}\\left( X_{b}\\cdot W-y\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化计算梯度\n",
    "def dJ_vector(W,X_b,y):\n",
    "    return X_b.T.dot(X_b.dot(W) - y) * 2. / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化版\n",
    "def gradient_descent_vector(init_w,X,y,eta=0.01,epsilon=1e-8,n_iters=1e4,init_b=0.):\n",
    "    \n",
    "    # 补1\n",
    "    X_b = np.hstack([np.ones((len(X),1)),X])\n",
    "    # 补b\n",
    "    W = np.hstack([init_b,init_w])\n",
    "    \n",
    "    i_iter=0\n",
    "    while i_iter<n_iters:\n",
    "        grad=dJ_vector(W,X_b,y)\n",
    "        last_W=W\n",
    "        W=W-eta*grad\n",
    "        if(abs(J(W,X_b,y)-J(last_W,X_b,y)) < epsilon):\n",
    "            break\n",
    "        i_iter += 1    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入真实数据进行训练并与正规方程解进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x=boston.data\n",
    "y=boston.target\n",
    "\n",
    "x=x[y<50]\n",
    "y=y[y<50]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=666)\n",
    "\n",
    "# 预测函数\n",
    "def predict_normal(W,X_test):\n",
    "        X_b = np.hstack([np.ones((len(X_test),1)),X_test])\n",
    "        y_hat = X_b.dot(W)\n",
    "        return y_hat\n",
    "\n",
    "# 引入R^2进行评价\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正规方程解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 ms, sys: 1.77 ms, total: 13.3 ms\n",
      "Wall time: 14.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.8129794056212809"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数: [-1.20354261e-01  3.64423279e-02 -3.61493155e-02  5.12978140e-02\n",
      " -1.15775825e+01  3.42740062e+00 -2.32311760e-02 -1.19487594e+00\n",
      "  2.60101728e-01 -1.40219119e-02 -8.35430488e-01  7.80472852e-03\n",
      " -3.80923751e-01]\n",
      "截距: 34.117399723229845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "%time lin_reg.fit(X_train,y_train)\n",
    "lin_reg.score(X_test,y_test)\n",
    "\n",
    "print(\"系数:\",lin_reg.coef_)\n",
    "print(\"截距:\",lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zc/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/zc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in square\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/zc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.42362e+01, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.93000e-01,\n",
       "        6.34300e+00, 1.00000e+02, 1.57410e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.96900e+02, 2.03200e+01],\n",
       "       [3.67822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 7.70000e-01,\n",
       "        5.36200e+00, 9.62000e+01, 2.10360e+00, 2.40000e+01, 6.66000e+02,\n",
       "        2.02000e+01, 3.80790e+02, 1.01900e+01],\n",
       "       [1.04690e-01, 4.00000e+01, 6.41000e+00, 1.00000e+00, 4.47000e-01,\n",
       "        7.26700e+00, 4.90000e+01, 4.78720e+00, 4.00000e+00, 2.54000e+02,\n",
       "        1.76000e+01, 3.89250e+02, 6.05000e+00],\n",
       "       [1.15172e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,\n",
       "        5.70100e+00, 9.50000e+01, 3.78720e+00, 4.00000e+00, 3.07000e+02,\n",
       "        2.10000e+01, 3.58770e+02, 1.83500e+01],\n",
       "       [6.58800e-02, 0.00000e+00, 2.46000e+00, 0.00000e+00, 4.88000e-01,\n",
       "        7.76500e+00, 8.33000e+01, 2.74100e+00, 3.00000e+00, 1.93000e+02,\n",
       "        1.78000e+01, 3.95560e+02, 7.56000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_w = np.zeros(X_train.shape[1])\n",
    "\n",
    "gradient_descent_vector(init_w,X_train,y_train)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 真实数据之间的差距较大，相比较我们学习率太大，所以结果无法收敛，我们应该降低学习率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27586818724477247"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数: [-0.10245704  0.11535876 -0.06248791  0.00207516  0.00447152  0.11954208\n",
      "  0.04684195  0.03460927 -0.00452122  0.00324507  0.1271939   0.04484862\n",
      " -0.22542441]\n",
      "截距: 0.011951606011577695\n"
     ]
    }
   ],
   "source": [
    "W_ = gradient_descent_vector(init_w,X_train,y_train,eta=1e-6)\n",
    "\n",
    "r2_score(y_test,predict_normal(W_,X_test))\n",
    "\n",
    "print(\"系数:\",W_[1:])\n",
    "print(\"截距:\",W_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 我们结果不是很好，有可能是学习率太小，使得我们在限定步数内梯度没有下降到最优点，我们尝试应该增大限定步数继续下降梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 123 ms, total: 1min 5s\n",
      "Wall time: 32.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7542932581943915"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数: [-1.07889200e-01  5.91494760e-02 -5.72920411e-02  1.19334353e-01\n",
      "  2.07223623e-01  3.91254775e+00  1.50564949e-03 -5.36511902e-01\n",
      "  1.13424276e-01 -9.76209406e-03  5.35544815e-02  1.58440412e-02\n",
      " -3.78786162e-01]\n",
      "截距: 0.43522123962172593\n"
     ]
    }
   ],
   "source": [
    "%time W_ = gradient_descent_vector(init_w,X_train,y_train,eta=1e-6,n_iters=1e6)\n",
    "\n",
    "r2_score(y_test,predict_normal(W_,X_test))\n",
    "\n",
    "print(\"系数:\",W_[1:])\n",
    "print(\"截距:\",W_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 现在我们准确率有所提升，但还不是很理想，而且训练耗时较长\n",
    "这很有可能是因为数据各个特征量级相差较大，我们应该尝试对数据做归一化处理**\n",
    "\n",
    "<br/>\n",
    " <img src=\"../img/5.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardScaler = StandardScaler()\n",
    "standardScaler.fit(X_train)\n",
    "\n",
    "X_train = standardScaler.transform(X_train)\n",
    "X_test = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 1.52 ms, total: 309 ms\n",
      "Wall time: 158 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8129873310487505"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数: [-1.04042202  0.83093351 -0.24794356  0.01179456 -1.35034756  2.25074\n",
      " -0.66384353 -2.53568774  2.25572406 -2.34011572 -1.76565394  0.70923397\n",
      " -2.72677064]\n",
      "截距: 21.500765306122382\n"
     ]
    }
   ],
   "source": [
    "%time W_ = gradient_descent_vector(init_w,X_train,y_train)\n",
    "\n",
    "r2_score(y_test,predict_normal(W_,X_test))\n",
    "\n",
    "print(\"系数:\",W_[1:])\n",
    "print(\"截距:\",W_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法的优势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 50000\n",
    "\n",
    "big_X = np.random.normal(size=(m, n))\n",
    "\n",
    "true_W = np.random.uniform(0.0, 100.0, size=n+1)\n",
    "\n",
    "big_y = big_X.dot(true_W[1:]) + true_W[0] + np.random.normal(0., 10., size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 539 ms, total: 6.89 s\n",
      "Wall time: 3.87 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "%time lin_reg.fit(big_X,big_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 s, sys: 128 ms, total: 2.3 s\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%time W_ = gradient_descent_vector(np.zeros(n),big_X,big_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 维数越大梯度下降法优势越明显，同时正规方程解的复杂度越大\n",
    "* 但样本数量很影响梯度下降法的性能，因为梯度计算的过程要每一个数据都要参与计算，所以我们将引入随机梯度下降法来优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
